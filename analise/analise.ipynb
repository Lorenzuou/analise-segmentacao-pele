{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib\n",
    "from matplotlib import pyplot as plt\n",
    "import cv2\n",
    "import supervision as sv\n",
    "import os\n",
    "from scipy.ndimage import distance_transform_edt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pixel_accuracy(pred, target):\n",
    "    correct = (pred == target).sum()\n",
    "    total = pred.size\n",
    "    return correct / total\n",
    "\n",
    "def intersection_over_union(pred, target):\n",
    "    intersection = (pred & target).sum()\n",
    "    union = (pred | target).sum()\n",
    "    return intersection / union\n",
    "\n",
    "def dice_coefficient(pred, target):\n",
    "    intersection = (pred & target).sum()\n",
    "    return 2 * intersection / (pred.sum() + target.sum())\n",
    "\n",
    "\n",
    "def nsd_coefficient(pred, ref, tau = 1):\n",
    "    # Get the surface (edges) of both segmentations\n",
    "    pred_surface = pred - distance_transform_edt(1 - pred) > 0\n",
    "    ref_surface = ref - distance_transform_edt(1 - ref) > 0\n",
    "\n",
    "    # Calculate the distance transform for each segmentation\n",
    "    dist_pred_to_ref = distance_transform_edt(1 - ref)\n",
    "    dist_ref_to_pred = distance_transform_edt(1 - pred)\n",
    "\n",
    "    # Check distances for each surface\n",
    "    within_tau_pred = dist_pred_to_ref[pred_surface] <= tau\n",
    "    within_tau_ref = dist_ref_to_pred[ref_surface] <= tau\n",
    "\n",
    "    # Compute the NSD\n",
    "    nsd_pred = np.sum(within_tau_pred) / np.sum(pred_surface)\n",
    "    nsd_ref = np.sum(within_tau_ref) / np.sum(ref_surface)\n",
    "    \n",
    "    nsd = 0.5 * (nsd_pred + nsd_ref)\n",
    "\n",
    "    return nsd\n",
    "\n",
    "def calculate_metrics(pred_list, target_list, metrics):\n",
    "    if metrics is None:\n",
    "        metrics = ['pixel_accuracy', 'iou', 'dice', 'nsd']\n",
    "        \n",
    "    results = {metric: 0 for metric in metrics}\n",
    "    n = len(pred_list)  # Assuming pred_list and target_list are of the same length\n",
    "\n",
    "    for pred, target in zip(pred_list, target_list):\n",
    "        if 'pixel_accuracy' in metrics:\n",
    "            results['pixel_accuracy'] += pixel_accuracy(pred, target)\n",
    "        if 'iou' in metrics:\n",
    "            results['iou'] += intersection_over_union(pred, target)\n",
    "        if 'dice' in metrics:\n",
    "            results['dice'] += dice_coefficient(pred, target)\n",
    "        if 'nsd' in metrics:\n",
    "            results['nsd'] += nsd_coefficient(pred, target)\n",
    "\n",
    "    # Calculate the mean for each metric\n",
    "    for metric in results:\n",
    "        results[metric] /= n\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "def get_segmentation_mask_from_image(img, shape=(1920, 1080)):\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    masks = img > 0\n",
    "    return masks\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "\n",
    "# Define Erosion Function\n",
    "def apply_erosion(image, kernel_size=(5, 5), iterations=1):\n",
    "    \"\"\"\n",
    "    Apply erosion to an image using a specified kernel size and number of iterations.\n",
    "    \n",
    "    Parameters:\n",
    "    - image: The input image on which erosion will be applied.\n",
    "    - kernel_size: The size of the kernel to be used for erosion.\n",
    "    - iterations: The number of times erosion is applied.\n",
    "    \n",
    "    Returns:\n",
    "    - eroded_image: The eroded image.\n",
    "    \"\"\n",
    "    \n",
    "    \"\"\"\n",
    "    est = cv2.getStructuringElement(cv2.MORPH_ERODE, kernel_size)\n",
    "    eroded_image = cv2.erode(image, est, iterations=iterations)\n",
    "    return eroded_image\n",
    "\n",
    "\n",
    "def erode_all_masks(masks: List[np.ndarray], kernel_size=(5, 5), iterations=1) -> List[np.ndarray]:\n",
    "    \"\"\"Apply erosion to all masks.\"\"\"\n",
    "    return [apply_erosion(mask, kernel_size, iterations) for mask in masks]\n",
    "\n",
    "def apply_dilation(image, kernel_size=(5, 5), iterations=1):\n",
    "    \"\"\"\n",
    "    Apply dilation to an image using a specified kernel size and number of iterations.\n",
    "    \n",
    "    Parameters:\n",
    "    - image: The input image on which dilation will be applied.\n",
    "    - kernel_size: The size of the kernel to be used for dilation.\n",
    "    - iterations: The number of times dilation is applied.\n",
    "    \n",
    "    Returns:\n",
    "    - dilated_image: The dilated image.\n",
    "    \"\"\n",
    "    \n",
    "    \"\"\"\n",
    "    est = cv2.getStructuringElement(cv2.MORPH_DILATE, kernel_size)\n",
    "    dilated_image = cv2.dilate(image, est, iterations=iterations)\n",
    "    return dilated_image\n",
    "\n",
    "def dilate_all_masks(masks: List[np.ndarray], kernel_size=(5, 5), iterations=1) -> List[np.ndarray]:\n",
    "    \"\"\"Apply dilation to all masks.\"\"\"\n",
    "    return [apply_dilation(mask, kernel_size, iterations) for mask in masks]\n",
    "\n",
    "\n",
    "def apply_opening(image, kernel_size=(5, 5), iterations=1):\n",
    "    \"\"\"\n",
    "    Apply opening to an image using a specified kernel size and number of iterations.\n",
    "    \n",
    "    Parameters:\n",
    "    - image: The input image on which opening will be applied.\n",
    "    - kernel_size: The size of the kernel to be used for opening.\n",
    "    - iterations: The number of times opening is applied.\n",
    "    \n",
    "    Returns:\n",
    "    - opened_image: The opened image.\n",
    "    \"\"\n",
    "    \n",
    "    \"\"\"\n",
    "    est = cv2.getStructuringElement(cv2.MORPH_OPEN, kernel_size)\n",
    "    opened_image = cv2.morphologyEx(image, cv2.MORPH_OPEN, est, iterations=iterations)\n",
    "    return opened_image\n",
    "\n",
    "def open_all_masks(masks: List[np.ndarray], kernel_size=(5, 5), iterations=1) -> List[np.ndarray]:\n",
    "    \"\"\"Apply opening to all masks.\"\"\"\n",
    "    return [apply_opening(mask, kernel_size, iterations) for mask in masks]\n",
    "\n",
    "def apply_closing(image, kernel_size=(5, 5), iterations=1):\n",
    "    \"\"\"\n",
    "    Apply closing to an image using a specified kernel size and number of iterations.\n",
    "    \n",
    "    Parameters:\n",
    "    - image: The input image on which closing will be applied.\n",
    "    - kernel_size: The size of the kernel to be used for closing.\n",
    "    - iterations: The number of times closing is applied.\n",
    "    \n",
    "    Returns:\n",
    "    - closed_image: The closed image.\n",
    "    \"\"\n",
    "    \n",
    "    \"\"\"\n",
    "    est = cv2.getStructuringElement(cv2.MORPH_RECT, kernel_size)\n",
    "    closed_image = cv2.morphologyEx(image, cv2.MORPH_CLOSE, est, iterations=iterations)\n",
    "    return closed_image\n",
    "\n",
    "def close_all_masks(masks: List[np.ndarray], kernel_size=(5, 5), iterations=1) -> List[np.ndarray]:\n",
    "    \"\"\"Apply closing to all masks.\"\"\"\n",
    "    return [apply_closing(mask, kernel_size, iterations) for mask in masks]\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_and_process_image(mask_path: str, original_mask_path: str) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"Load and preprocess mask images.\"\"\"\n",
    "    mask = cv2.imread(mask_path)\n",
    "    original_mask_img = cv2.imread(original_mask_path)\n",
    "    \n",
    "    # Resize logic\n",
    "    if original_mask_img.shape[:2] != mask.shape[:2]:\n",
    "        target_shape = (\n",
    "            min(original_mask_img.shape[1], mask.shape[1]),\n",
    "            min(original_mask_img.shape[0], mask.shape[0])\n",
    "        )\n",
    "        original_mask_img = cv2.resize(original_mask_img, target_shape[::-1])\n",
    "        mask = cv2.resize(mask, target_shape[::-1])\n",
    "    \n",
    "    return get_segmentation_mask_from_image(mask, original_mask_img.shape), \\\n",
    "           get_segmentation_mask_from_image(original_mask_img, original_mask_img.shape)\n",
    "\n",
    "\n",
    "def load_and_process_image(mask_path: str, original_mask_path: str) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"Load and preprocess mask images.\"\"\"\n",
    "    mask = cv2.imread(mask_path)\n",
    "    original_mask_img = cv2.imread(original_mask_path)\n",
    "    \n",
    "    # Resize logic\n",
    "    if original_mask_img.shape[:2] != mask.shape[:2]:\n",
    "        target_shape = (\n",
    "            min(original_mask_img.shape[1], mask.shape[1]),\n",
    "            min(original_mask_img.shape[0], mask.shape[0])\n",
    "        )\n",
    "        original_mask_img = cv2.resize(original_mask_img, target_shape[::-1])\n",
    "        mask = cv2.resize(mask, target_shape[::-1])\n",
    "    \n",
    "    return mask, original_mask_img\n",
    "\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "def collect_metrics(sessoes_folder: str, original_mask_folder: str) -> Tuple[pd.DataFrame, Dict]:\n",
    "    \"\"\"Collect metrics for all images and organize them into a DataFrame.\"\"\"\n",
    "    # Initialize data storage\n",
    "    data_rows = []\n",
    "    groups_data = {\n",
    "        \"sam\": {f\"group_{i}\": {\"metrics\": [], \"time\": []} for i in range(1, 5)},\n",
    "        \"manual\": {f\"group_{i}\": {\"metrics\": [], \"time\": []} for i in range(1, 5)}\n",
    "    }\n",
    "    \n",
    "    print(f\"Processing folders:\")\n",
    "    print(f\"Total folders: {len(os.listdir(sessoes_folder))}\")\n",
    "    print(f\"SAM folders: {len([f for f in os.listdir(sessoes_folder) if 'sam' in f])}\")\n",
    "    print(f\"Manual folders: {len([f for f in os.listdir(sessoes_folder) if 'manual' in f])}\\n\")\n",
    "\n",
    "    progress = 0\n",
    "    total_folders = len(os.listdir(sessoes_folder))\n",
    "\n",
    "    for sessao in tqdm(os.listdir(sessoes_folder)):\n",
    "        # Get group name\n",
    "        group_name = next((g for g in [\"group_1\", \"group_2\", \"group_3\", \"group_4\"] if g in sessao), None)\n",
    "        if not group_name:\n",
    "            continue\n",
    "            \n",
    "        # Skip small sessions\n",
    "        masked_path = os.path.join(sessoes_folder, sessao, \"masked\")\n",
    "        if len(os.listdir(masked_path)) < 50:\n",
    "            print(f\"Skipping {sessao} - insufficient images\")\n",
    "            continue\n",
    "            \n",
    "        # Get processing type (sam or manual)\n",
    "        proc_type = \"sam\" if \"sam\" in sessao else \"manual\"\n",
    "        print(f\"Processing {sessao} ({progress}/{total_folders})\")\n",
    "    \n",
    "        \n",
    "        # Process each image\n",
    "        for image in tqdm(os.listdir(masked_path)):\n",
    "            original_mask_name = image.replace('.jpg', '_segmentation.png')\n",
    "            original_mask_path = os.path.join(original_mask_folder, original_mask_name)\n",
    "\n",
    "            \n",
    "            \n",
    "            if not os.path.exists(original_mask_path):\n",
    "                continue\n",
    "                \n",
    "            # Process images\n",
    "            mask, original_mask = load_and_process_image(\n",
    "                os.path.join(masked_path, image),\n",
    "                original_mask_path\n",
    "            )\n",
    "\n",
    "            \n",
    "            # Calculate metrics\n",
    "            pixel_acc, iou, dice, nsd = calculate_metrics([get_segmentation_mask_from_image(mask)],\n",
    "                                                          [get_segmentation_mask_from_image(original_mask)],\n",
    "                                                          metrics=['pixel_accuracy', 'iou', 'dice', 'nsd']).values()\n",
    "\n",
    "\n",
    "            # Apply morphological operations\n",
    "            eroded_mask = apply_erosion(mask, kernel_size=(5, 5), iterations=1)\n",
    "            dilated_mask = apply_dilation(mask, kernel_size=(5, 5), iterations=1)\n",
    "            opened_mask = apply_opening(mask, kernel_size=(5, 5), iterations=1)\n",
    "            closed_mask = apply_closing(mask, kernel_size=(5, 5), iterations=1)\n",
    "            \n",
    "            # Calculate metrics for morphological operations\n",
    "            eroded_metrics = calculate_metrics([eroded_mask], [original_mask], metrics=['pixel_accuracy', 'iou', 'dice', 'nsd'])\n",
    "            dilated_metrics = calculate_metrics([dilated_mask], [original_mask], metrics=['pixel_accuracy', 'iou', 'dice', 'nsd'])\n",
    "            opened_metrics = calculate_metrics([opened_mask], [original_mask], metrics=['pixel_accuracy', 'iou', 'dice', 'nsd'])\n",
    "            closed_metrics = calculate_metrics([closed_mask], [original_mask], metrics=['pixel_accuracy', 'iou', 'dice', 'nsd'])\n",
    "            \n",
    "            \n",
    "            \n",
    "            # Get processing time\n",
    "            with open(os.path.join(sessoes_folder, sessao, \"time.txt\"), \"r\") as f:\n",
    "                time = float(f.readlines()[0])\n",
    "            \n",
    "            # Store data\n",
    "            data_rows.append({\n",
    "                \"group\": group_name,\n",
    "                \"type\": proc_type,\n",
    "                \"pixel_accuracy\": pixel_acc,\n",
    "                \"iou\": iou,\n",
    "                \"dice\": dice,\n",
    "                \"nsd\": nsd,\n",
    "                \"eroded_pixel_accuracy\": eroded_metrics['pixel_accuracy'],\n",
    "                \"eroded_iou\": eroded_metrics['iou'],\n",
    "                \"eroded_dice\": eroded_metrics['dice'],\n",
    "                \"eroded_nsd\": eroded_metrics['nsd'],\n",
    "                \"dilated_pixel_accuracy\": dilated_metrics['pixel_accuracy'],\n",
    "                \"dilated_iou\": dilated_metrics['iou'],\n",
    "                \"dilated_dice\": dilated_metrics['dice'],\n",
    "                \"dilated_nsd\": dilated_metrics['nsd'],\n",
    "                \"opened_pixel_accuracy\": opened_metrics['pixel_accuracy'],\n",
    "                \"opened_iou\": opened_metrics['iou'],\n",
    "                \"opened_dice\": opened_metrics['dice'],\n",
    "                \"opened_nsd\": opened_metrics['nsd'],\n",
    "                \"time\": time,\n",
    "                \"image_name\": image\n",
    "            })\n",
    "            \n",
    "            # Store group-specific data\n",
    "            groups_data[proc_type][group_name][\"metrics\"].append([pixel_acc, iou, dice, nsd])\n",
    "            groups_data[proc_type][group_name][\"time\"].append(time)\n",
    "        progress += 1\n",
    "    return pd.DataFrame(data_rows), groups_data\n",
    "\n",
    "def visualize_overall_metrics(df: pd.DataFrame):\n",
    "    \"\"\"Create visualizations for overall metrics comparison.\"\"\"\n",
    "    plt.figure(figsize=(15, 10))\n",
    "    \n",
    "    # Create boxplots for each metric\n",
    "    metrics = ['pixel_accuracy', 'iou', 'dice']\n",
    "    for i, metric in enumerate(metrics, 1):\n",
    "        plt.subplot(2, 2, i)\n",
    "        sns.boxplot(data=df, x='type', y=metric)\n",
    "        plt.title(f'{metric.replace(\"_\", \" \").title()} Distribution')\n",
    "        plt.xlabel('Processing Type')\n",
    "        plt.ylabel(metric.replace(\"_\", \" \").title())\n",
    "    \n",
    "    # Create time comparison\n",
    "    plt.subplot(2, 2, 4)\n",
    "    sns.boxplot(data=df, x='type', y='time')\n",
    "    plt.title('Processing Time Distribution')\n",
    "    plt.xlabel('Processing Type')\n",
    "    plt.ylabel('Time (seconds)')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def visualize_group_metrics(df: pd.DataFrame):\n",
    "    \"\"\"Create visualizations for group-specific metrics.\"\"\"\n",
    "    metrics = ['pixel_accuracy', 'iou', 'dice', 'time']\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "    fig.suptitle('Metrics by Group and Processing Type')\n",
    "    \n",
    "    for i, metric in enumerate(metrics):\n",
    "        ax = axes[i // 2, i % 2]\n",
    "        sns.boxplot(data=df, x='group', y=metric, hue='type', ax=ax)\n",
    "        ax.set_title(f'{metric.replace(\"_\", \" \").title()}')\n",
    "        ax.set_xlabel('Group')\n",
    "        ax.set_ylabel(metric.replace(\"_\", \" \").title())\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def display_summary_stats(df: pd.DataFrame):\n",
    "    \"\"\"Display summary statistics in a clean table format.\"\"\"\n",
    "    # Overall statistics\n",
    "    overall_stats = df.groupby('type')[['pixel_accuracy', 'iou', 'dice', 'time']].agg(['mean', 'std', 'count'])\n",
    "    \n",
    "    # Group statistics\n",
    "    group_stats = df.groupby(['type', 'group'])[['pixel_accuracy', 'iou', 'dice', 'time']].agg(['mean', 'std', 'count'])\n",
    "    \n",
    "    return overall_stats, group_stats\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing folders:\n",
      "Total folders: 100\n",
      "SAM folders: 50\n",
      "Manual folders: 50\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/100 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing manual-k9g8yb-group_2-Isaías_Correia_Altoé (0/100)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "RuntimeWarning: invalid value encountered in scalar divide\n",
      " 92%|█████████▏| 46/50 [00:52<00:04,  1.14s/it]\n",
      "  0%|          | 0/100 [00:52<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m original_mask_folder \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpad_segmentation_all/all-mask\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Collect and process data\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m df, groups_data \u001b[38;5;241m=\u001b[39m \u001b[43mcollect_metrics\u001b[49m\u001b[43m(\u001b[49m\u001b[43msessoes_folder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moriginal_mask_folder\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Generate and display statistics\u001b[39;00m\n\u001b[1;32m      9\u001b[0m overall_stats, group_stats \u001b[38;5;241m=\u001b[39m display_summary_stats(df)\n",
      "Cell \u001b[0;32mIn[17], line 90\u001b[0m, in \u001b[0;36mcollect_metrics\u001b[0;34m(sessoes_folder, original_mask_folder)\u001b[0m\n\u001b[1;32m     83\u001b[0m mask, original_mask \u001b[38;5;241m=\u001b[39m load_and_process_image(\n\u001b[1;32m     84\u001b[0m     os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(masked_path, image),\n\u001b[1;32m     85\u001b[0m     original_mask_path\n\u001b[1;32m     86\u001b[0m )\n\u001b[1;32m     89\u001b[0m \u001b[38;5;66;03m# Calculate metrics\u001b[39;00m\n\u001b[0;32m---> 90\u001b[0m pixel_acc, iou, dice, nsd \u001b[38;5;241m=\u001b[39m \u001b[43mcalculate_metrics\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mget_segmentation_mask_from_image\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmask\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     91\u001b[0m \u001b[43m                                              \u001b[49m\u001b[43m[\u001b[49m\u001b[43mget_segmentation_mask_from_image\u001b[49m\u001b[43m(\u001b[49m\u001b[43moriginal_mask\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     92\u001b[0m \u001b[43m                                              \u001b[49m\u001b[43mmetrics\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpixel_accuracy\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43miou\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdice\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mnsd\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mvalues()\n\u001b[1;32m     95\u001b[0m \u001b[38;5;66;03m# Apply morphological operations\u001b[39;00m\n\u001b[1;32m     96\u001b[0m eroded_mask \u001b[38;5;241m=\u001b[39m apply_erosion(mask, kernel_size\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m5\u001b[39m, \u001b[38;5;241m5\u001b[39m), iterations\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "Cell \u001b[0;32mIn[9], line 52\u001b[0m, in \u001b[0;36mcalculate_metrics\u001b[0;34m(pred_list, target_list, metrics)\u001b[0m\n\u001b[1;32m     50\u001b[0m         results[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdice\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m dice_coefficient(pred, target)\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnsd\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m metrics:\n\u001b[0;32m---> 52\u001b[0m         results[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnsd\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mnsd_coefficient\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpred\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;66;03m# Calculate the mean for each metric\u001b[39;00m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m metric \u001b[38;5;129;01min\u001b[39;00m results:\n",
      "Cell \u001b[0;32mIn[9], line 22\u001b[0m, in \u001b[0;36mnsd_coefficient\u001b[0;34m(pred, ref, tau)\u001b[0m\n\u001b[1;32m     19\u001b[0m ref_surface \u001b[38;5;241m=\u001b[39m ref \u001b[38;5;241m-\u001b[39m distance_transform_edt(\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m ref) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# Calculate the distance transform for each segmentation\u001b[39;00m\n\u001b[0;32m---> 22\u001b[0m dist_pred_to_ref \u001b[38;5;241m=\u001b[39m \u001b[43mdistance_transform_edt\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mref\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     23\u001b[0m dist_ref_to_pred \u001b[38;5;241m=\u001b[39m distance_transform_edt(\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m pred)\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m# Check distances for each surface\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/gradio/lib/python3.9/site-packages/scipy/ndimage/_morphology.py:2293\u001b[0m, in \u001b[0;36mdistance_transform_edt\u001b[0;34m(input, sampling, return_distances, return_indices, distances, indices)\u001b[0m\n\u001b[1;32m   2290\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2291\u001b[0m     ft \u001b[38;5;241m=\u001b[39m numpy\u001b[38;5;241m.\u001b[39mzeros((\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mndim,) \u001b[38;5;241m+\u001b[39m \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mshape, dtype\u001b[38;5;241m=\u001b[39mnumpy\u001b[38;5;241m.\u001b[39mint32)\n\u001b[0;32m-> 2293\u001b[0m \u001b[43m_nd_image\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meuclidean_feature_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msampling\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mft\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2294\u001b[0m \u001b[38;5;66;03m# if requested, calculate the distance transform\u001b[39;00m\n\u001b[1;32m   2295\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m return_distances:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Set paths\n",
    "sessoes_folder = \"./sessoes\"\n",
    "original_mask_folder = 'pad_segmentation_all/all-mask'\n",
    "\n",
    "# Collect and process data\n",
    "df, groups_data = collect_metrics(sessoes_folder, original_mask_folder)\n",
    "\n",
    "# Generate and display statistics\n",
    "overall_stats, group_stats = display_summary_stats(df)\n",
    "\n",
    "print(\"Overall Statistics:\")\n",
    "display(overall_stats)\n",
    "print(\"\\nGroup Statistics:\")\n",
    "display(group_stats)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test nsd on a single image\n",
    "reference_image = cv2.imread('pad_segmentation_all/all-mask/888da860-558d-487f-aeb6-bc114610a298_segmentation.png')\n",
    "reference_image = cv2.cvtColor(reference_image, cv2.COLOR_BGR2RGB)\n",
    "reference_image = reference_image > 0\n",
    "reference_image = reference_image.astype(np.uint8)\n",
    "\n",
    "test_image = cv2.imread('sessoes/group_2-manual-it3uyi-group_2-Marcel_Scaramussa/masked/888da860-558d-487f-aeb6-bc114610a298.jpg')\n",
    "test_image = cv2.cvtColor(test_image, cv2.COLOR_BGR2RGB)\n",
    "test_image = test_image > 0\n",
    "test_image = test_image.astype(np.uint8)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.to_csv('metrics_groups_2.csv', index=False)\n",
    "df = pd.read_csv('metrics.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "overall_stats, group_stats = display_summary_stats(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall Statistics:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"3\" halign=\"left\">pixel_accuracy</th>\n",
       "      <th colspan=\"3\" halign=\"left\">iou</th>\n",
       "      <th colspan=\"3\" halign=\"left\">dice</th>\n",
       "      <th colspan=\"3\" halign=\"left\">time</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>type</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>manual</th>\n",
       "      <td>0.944708</td>\n",
       "      <td>0.050723</td>\n",
       "      <td>2500</td>\n",
       "      <td>0.738960</td>\n",
       "      <td>0.153104</td>\n",
       "      <td>2500</td>\n",
       "      <td>0.839592</td>\n",
       "      <td>0.118255</td>\n",
       "      <td>2500</td>\n",
       "      <td>480.843060</td>\n",
       "      <td>241.588619</td>\n",
       "      <td>2500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sam</th>\n",
       "      <td>0.917034</td>\n",
       "      <td>0.183580</td>\n",
       "      <td>2500</td>\n",
       "      <td>0.745614</td>\n",
       "      <td>0.222507</td>\n",
       "      <td>2500</td>\n",
       "      <td>0.828491</td>\n",
       "      <td>0.205885</td>\n",
       "      <td>2500</td>\n",
       "      <td>405.626657</td>\n",
       "      <td>232.740288</td>\n",
       "      <td>2500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       pixel_accuracy                       iou                      dice  \\\n",
       "                 mean       std count      mean       std count      mean   \n",
       "type                                                                        \n",
       "manual       0.944708  0.050723  2500  0.738960  0.153104  2500  0.839592   \n",
       "sam          0.917034  0.183580  2500  0.745614  0.222507  2500  0.828491   \n",
       "\n",
       "                              time                    \n",
       "             std count        mean         std count  \n",
       "type                                                  \n",
       "manual  0.118255  2500  480.843060  241.588619  2500  \n",
       "sam     0.205885  2500  405.626657  232.740288  2500  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "print(\"Overall Statistics:\")\n",
    "display(overall_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Group Statistics:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th colspan=\"3\" halign=\"left\">pixel_accuracy</th>\n",
       "      <th colspan=\"3\" halign=\"left\">iou</th>\n",
       "      <th colspan=\"3\" halign=\"left\">dice</th>\n",
       "      <th colspan=\"3\" halign=\"left\">time</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>type</th>\n",
       "      <th>group</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"4\" valign=\"top\">manual</th>\n",
       "      <th>group_1</th>\n",
       "      <td>0.942631</td>\n",
       "      <td>0.048388</td>\n",
       "      <td>550</td>\n",
       "      <td>0.706563</td>\n",
       "      <td>0.162286</td>\n",
       "      <td>550</td>\n",
       "      <td>0.816021</td>\n",
       "      <td>0.128011</td>\n",
       "      <td>550</td>\n",
       "      <td>504.460182</td>\n",
       "      <td>232.517458</td>\n",
       "      <td>550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>group_2</th>\n",
       "      <td>0.935099</td>\n",
       "      <td>0.061688</td>\n",
       "      <td>600</td>\n",
       "      <td>0.732269</td>\n",
       "      <td>0.157810</td>\n",
       "      <td>600</td>\n",
       "      <td>0.834269</td>\n",
       "      <td>0.124392</td>\n",
       "      <td>600</td>\n",
       "      <td>449.367549</td>\n",
       "      <td>291.164007</td>\n",
       "      <td>600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>group_3</th>\n",
       "      <td>0.955475</td>\n",
       "      <td>0.041909</td>\n",
       "      <td>700</td>\n",
       "      <td>0.782389</td>\n",
       "      <td>0.126559</td>\n",
       "      <td>700</td>\n",
       "      <td>0.871161</td>\n",
       "      <td>0.096744</td>\n",
       "      <td>700</td>\n",
       "      <td>499.354180</td>\n",
       "      <td>176.916020</td>\n",
       "      <td>700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>group_4</th>\n",
       "      <td>0.943741</td>\n",
       "      <td>0.048035</td>\n",
       "      <td>650</td>\n",
       "      <td>0.725781</td>\n",
       "      <td>0.156860</td>\n",
       "      <td>650</td>\n",
       "      <td>0.830452</td>\n",
       "      <td>0.118185</td>\n",
       "      <td>650</td>\n",
       "      <td>469.978607</td>\n",
       "      <td>255.285750</td>\n",
       "      <td>650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"4\" valign=\"top\">sam</th>\n",
       "      <th>group_1</th>\n",
       "      <td>0.952675</td>\n",
       "      <td>0.109373</td>\n",
       "      <td>550</td>\n",
       "      <td>0.772458</td>\n",
       "      <td>0.195436</td>\n",
       "      <td>550</td>\n",
       "      <td>0.853237</td>\n",
       "      <td>0.169997</td>\n",
       "      <td>550</td>\n",
       "      <td>428.067545</td>\n",
       "      <td>205.044084</td>\n",
       "      <td>550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>group_2</th>\n",
       "      <td>0.835480</td>\n",
       "      <td>0.288749</td>\n",
       "      <td>600</td>\n",
       "      <td>0.677822</td>\n",
       "      <td>0.285938</td>\n",
       "      <td>600</td>\n",
       "      <td>0.761449</td>\n",
       "      <td>0.276931</td>\n",
       "      <td>600</td>\n",
       "      <td>432.748965</td>\n",
       "      <td>243.439248</td>\n",
       "      <td>600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>group_3</th>\n",
       "      <td>0.944061</td>\n",
       "      <td>0.104506</td>\n",
       "      <td>700</td>\n",
       "      <td>0.764853</td>\n",
       "      <td>0.185946</td>\n",
       "      <td>700</td>\n",
       "      <td>0.849942</td>\n",
       "      <td>0.163458</td>\n",
       "      <td>700</td>\n",
       "      <td>340.951425</td>\n",
       "      <td>166.746893</td>\n",
       "      <td>700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>group_4</th>\n",
       "      <td>0.933051</td>\n",
       "      <td>0.150548</td>\n",
       "      <td>650</td>\n",
       "      <td>0.764758</td>\n",
       "      <td>0.200001</td>\n",
       "      <td>650</td>\n",
       "      <td>0.846337</td>\n",
       "      <td>0.183639</td>\n",
       "      <td>650</td>\n",
       "      <td>431.252484</td>\n",
       "      <td>286.516117</td>\n",
       "      <td>650</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               pixel_accuracy                       iou                  \\\n",
       "                         mean       std count      mean       std count   \n",
       "type   group                                                              \n",
       "manual group_1       0.942631  0.048388   550  0.706563  0.162286   550   \n",
       "       group_2       0.935099  0.061688   600  0.732269  0.157810   600   \n",
       "       group_3       0.955475  0.041909   700  0.782389  0.126559   700   \n",
       "       group_4       0.943741  0.048035   650  0.725781  0.156860   650   \n",
       "sam    group_1       0.952675  0.109373   550  0.772458  0.195436   550   \n",
       "       group_2       0.835480  0.288749   600  0.677822  0.285938   600   \n",
       "       group_3       0.944061  0.104506   700  0.764853  0.185946   700   \n",
       "       group_4       0.933051  0.150548   650  0.764758  0.200001   650   \n",
       "\n",
       "                    dice                        time                    \n",
       "                    mean       std count        mean         std count  \n",
       "type   group                                                            \n",
       "manual group_1  0.816021  0.128011   550  504.460182  232.517458   550  \n",
       "       group_2  0.834269  0.124392   600  449.367549  291.164007   600  \n",
       "       group_3  0.871161  0.096744   700  499.354180  176.916020   700  \n",
       "       group_4  0.830452  0.118185   650  469.978607  255.285750   650  \n",
       "sam    group_1  0.853237  0.169997   550  428.067545  205.044084   550  \n",
       "       group_2  0.761449  0.276931   600  432.748965  243.439248   600  \n",
       "       group_3  0.849942  0.163458   700  340.951425  166.746893   700  \n",
       "       group_4  0.846337  0.183639   650  431.252484  286.516117   650  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"\\nGroup Statistics:\")\n",
    "display(group_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Dict, List, Tuple, Set\n",
    "\n",
    "def get_common_images(sessoes_folder: str) -> Set[str]:\n",
    "    \"\"\"Get list of images that are common across all groups.\"\"\"\n",
    "    image_sets = []\n",
    "    for sessao in os.listdir(sessoes_folder):\n",
    "        image_folder = os.path.join(sessoes_folder, sessao, \"masked\")\n",
    "        images = os.listdir(image_folder)\n",
    "        image_sets.append(set(images))\n",
    "    \n",
    "    common_images = set.intersection(*image_sets)\n",
    "    print(f\"Found {len(common_images)} common images across all groups\")\n",
    "    return common_images\n",
    "\n",
    "def collect_common_metrics(sessoes_folder: str, original_mask_folder: str, common_images: Set[str]) -> pd.DataFrame:\n",
    "    \"\"\"Collect metrics for common images across all groups.\"\"\"\n",
    "    data_rows = []\n",
    "    \n",
    "    # Process each session\n",
    "    for sessao in os.listdir(sessoes_folder):\n",
    "        # Get group name and processing type\n",
    "        group_name = next((g for g in [\"group_1\", \"group_2\", \"group_3\", \"group_4\"] if g in sessao), None)\n",
    "        if not group_name:\n",
    "            continue\n",
    "            \n",
    "        proc_type = \"sam\" if \"sam\" in sessao else \"manual\"\n",
    "        \n",
    "        # Process common images\n",
    "        for image in common_images:\n",
    "            mask_path = os.path.join(sessoes_folder, sessao, \"masked\", image)\n",
    "            original_mask_name = image.replace('.jpg', '_segmentation.png')\n",
    "            original_mask_path = os.path.join(original_mask_folder, original_mask_name)\n",
    "            \n",
    "            if not os.path.exists(original_mask_path) or not os.path.exists(mask_path):\n",
    "                continue\n",
    "                \n",
    "            # Load and process images\n",
    "            mask = cv2.imread(mask_path)\n",
    "            original_mask_img = cv2.imread(original_mask_path)\n",
    "            \n",
    "            # Resize if necessary\n",
    "            if original_mask_img.shape[:2] != mask.shape[:2]:\n",
    "                target_shape = (\n",
    "                    min(original_mask_img.shape[1], mask.shape[1]),\n",
    "                    min(original_mask_img.shape[0], mask.shape[0])\n",
    "                )\n",
    "                original_mask_img = cv2.resize(original_mask_img, target_shape[::-1])\n",
    "                mask = cv2.resize(mask, target_shape[::-1])\n",
    "            \n",
    "            # Get masks and calculate metrics\n",
    "            mask_processed = m.get_segmentation_mask_from_image(mask, original_mask_img.shape)\n",
    "            original_mask = m.get_segmentation_mask_from_image(original_mask_img, original_mask_img.shape)\n",
    "            pixel_acc, iou, dice = m.calculate_metrics([mask_processed], [original_mask])\n",
    "            \n",
    "            # Get processing time\n",
    "            with open(os.path.join(sessoes_folder, sessao, \"time.txt\"), \"r\") as f:\n",
    "                time = float(f.readlines()[0])\n",
    "            \n",
    "            # Store data\n",
    "            data_rows.append({\n",
    "                \"image\": image,\n",
    "                \"group\": group_name,\n",
    "                \"type\": proc_type,\n",
    "                \"pixel_accuracy\": pixel_acc,\n",
    "                \"iou\": iou,\n",
    "                \"dice\": dice,\n",
    "                \"time\": time\n",
    "            })\n",
    "    \n",
    "    return pd.DataFrame(data_rows)\n",
    "\n",
    "def visualize_common_metrics(df: pd.DataFrame):\n",
    "    \"\"\"Create visualizations for metrics of common images.\"\"\"\n",
    "    # Overall comparison\n",
    "    plt.figure(figsize=(15, 10))\n",
    "    metrics = ['pixel_accuracy', 'iou', 'dice', 'time']\n",
    "    \n",
    "    for i, metric in enumerate(metrics, 1):\n",
    "        plt.subplot(2, 2, i)\n",
    "        sns.boxplot(data=df, x='type', y=metric)\n",
    "        plt.title(f'{metric.replace(\"_\", \" \").title()} - Common Images')\n",
    "        plt.xlabel('Processing Type')\n",
    "        plt.ylabel(metric.replace(\"_\", \" \").title())\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Group-specific comparison\n",
    "    plt.figure(figsize=(15, 12))\n",
    "    for i, metric in enumerate(metrics, 1):\n",
    "        plt.subplot(2, 2, i)\n",
    "        sns.boxplot(data=df, x='group', y=metric, hue='type')\n",
    "        plt.title(f'{metric.replace(\"_\", \" \").title()} by Group - Common Images')\n",
    "        plt.xlabel('Group')\n",
    "        plt.ylabel(metric.replace(\"_\", \" \").title())\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Image-specific analysis\n",
    "    plt.figure(figsize=(15, 6))\n",
    "    avg_metrics = df.groupby('image')[['pixel_accuracy', 'iou', 'dice']].mean()\n",
    "    sns.heatmap(avg_metrics.T, cmap='YlOrRd', center=0.5)\n",
    "    plt.title('Average Metrics per Image')\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 25 common images across all groups\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Set paths\n",
    "sessoes_folder = \"./sessoes\"\n",
    "original_mask_folder = 'pad_segmentation_all/all-mask'\n",
    "\n",
    "# Get common images\n",
    "common_images = get_common_images(sessoes_folder)\n",
    "\n",
    "# Collect metrics for common images\n",
    "df = collect_common_metrics(sessoes_folder, original_mask_folder, common_images)\n",
    "\n",
    "# Calculate and display summary statistics\n",
    "overall_stats = df.groupby('type')[['pixel_accuracy', 'iou', 'dice', 'time']].agg(['mean', 'std', 'count'])\n",
    "group_stats = df.groupby(['type', 'group'])[['pixel_accuracy', 'iou', 'dice', 'time']].agg(['mean', 'std'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Overall Statistics for Common Images:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"3\" halign=\"left\">pixel_accuracy</th>\n",
       "      <th colspan=\"3\" halign=\"left\">iou</th>\n",
       "      <th colspan=\"3\" halign=\"left\">dice</th>\n",
       "      <th colspan=\"3\" halign=\"left\">time</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>type</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>manual</th>\n",
       "      <td>0.947617</td>\n",
       "      <td>0.045419</td>\n",
       "      <td>1250</td>\n",
       "      <td>0.753830</td>\n",
       "      <td>0.143847</td>\n",
       "      <td>1250</td>\n",
       "      <td>0.850952</td>\n",
       "      <td>0.106849</td>\n",
       "      <td>1250</td>\n",
       "      <td>480.843060</td>\n",
       "      <td>241.636970</td>\n",
       "      <td>1250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sam</th>\n",
       "      <td>0.914885</td>\n",
       "      <td>0.190079</td>\n",
       "      <td>1250</td>\n",
       "      <td>0.746352</td>\n",
       "      <td>0.230084</td>\n",
       "      <td>1250</td>\n",
       "      <td>0.826890</td>\n",
       "      <td>0.215138</td>\n",
       "      <td>1250</td>\n",
       "      <td>405.626657</td>\n",
       "      <td>232.786869</td>\n",
       "      <td>1250</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       pixel_accuracy                       iou                      dice  \\\n",
       "                 mean       std count      mean       std count      mean   \n",
       "type                                                                        \n",
       "manual       0.947617  0.045419  1250  0.753830  0.143847  1250  0.850952   \n",
       "sam          0.914885  0.190079  1250  0.746352  0.230084  1250  0.826890   \n",
       "\n",
       "                              time                    \n",
       "             std count        mean         std count  \n",
       "type                                                  \n",
       "manual  0.106849  1250  480.843060  241.636970  1250  \n",
       "sam     0.215138  1250  405.626657  232.786869  1250  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "print(\"\\nOverall Statistics for Common Images:\")\n",
    "display(overall_stats)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Group Statistics for Common Images:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th colspan=\"2\" halign=\"left\">pixel_accuracy</th>\n",
       "      <th colspan=\"2\" halign=\"left\">iou</th>\n",
       "      <th colspan=\"2\" halign=\"left\">dice</th>\n",
       "      <th colspan=\"2\" halign=\"left\">time</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>type</th>\n",
       "      <th>group</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"4\" valign=\"top\">manual</th>\n",
       "      <th>group_1</th>\n",
       "      <td>0.937957</td>\n",
       "      <td>0.050003</td>\n",
       "      <td>0.722362</td>\n",
       "      <td>0.156752</td>\n",
       "      <td>0.827884</td>\n",
       "      <td>0.121476</td>\n",
       "      <td>504.460182</td>\n",
       "      <td>232.729512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>group_2</th>\n",
       "      <td>0.945846</td>\n",
       "      <td>0.048915</td>\n",
       "      <td>0.752028</td>\n",
       "      <td>0.143517</td>\n",
       "      <td>0.849972</td>\n",
       "      <td>0.104602</td>\n",
       "      <td>449.367549</td>\n",
       "      <td>291.407353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>group_3</th>\n",
       "      <td>0.961580</td>\n",
       "      <td>0.029095</td>\n",
       "      <td>0.790642</td>\n",
       "      <td>0.121922</td>\n",
       "      <td>0.877057</td>\n",
       "      <td>0.089877</td>\n",
       "      <td>499.354180</td>\n",
       "      <td>177.042705</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>group_4</th>\n",
       "      <td>0.942388</td>\n",
       "      <td>0.048968</td>\n",
       "      <td>0.742476</td>\n",
       "      <td>0.146732</td>\n",
       "      <td>0.843263</td>\n",
       "      <td>0.107169</td>\n",
       "      <td>469.978607</td>\n",
       "      <td>255.482654</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"4\" valign=\"top\">sam</th>\n",
       "      <th>group_1</th>\n",
       "      <td>0.945942</td>\n",
       "      <td>0.120077</td>\n",
       "      <td>0.769443</td>\n",
       "      <td>0.199073</td>\n",
       "      <td>0.849936</td>\n",
       "      <td>0.179750</td>\n",
       "      <td>428.067545</td>\n",
       "      <td>205.231083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>group_2</th>\n",
       "      <td>0.831864</td>\n",
       "      <td>0.299137</td>\n",
       "      <td>0.678832</td>\n",
       "      <td>0.297463</td>\n",
       "      <td>0.758101</td>\n",
       "      <td>0.289651</td>\n",
       "      <td>432.748965</td>\n",
       "      <td>243.642707</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>group_3</th>\n",
       "      <td>0.942789</td>\n",
       "      <td>0.120787</td>\n",
       "      <td>0.767988</td>\n",
       "      <td>0.196788</td>\n",
       "      <td>0.849594</td>\n",
       "      <td>0.176268</td>\n",
       "      <td>340.951425</td>\n",
       "      <td>166.866296</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>group_4</th>\n",
       "      <td>0.935191</td>\n",
       "      <td>0.142607</td>\n",
       "      <td>0.765839</td>\n",
       "      <td>0.204035</td>\n",
       "      <td>0.846438</td>\n",
       "      <td>0.185568</td>\n",
       "      <td>431.252484</td>\n",
       "      <td>286.737109</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               pixel_accuracy                 iou                dice  \\\n",
       "                         mean       std      mean       std      mean   \n",
       "type   group                                                            \n",
       "manual group_1       0.937957  0.050003  0.722362  0.156752  0.827884   \n",
       "       group_2       0.945846  0.048915  0.752028  0.143517  0.849972   \n",
       "       group_3       0.961580  0.029095  0.790642  0.121922  0.877057   \n",
       "       group_4       0.942388  0.048968  0.742476  0.146732  0.843263   \n",
       "sam    group_1       0.945942  0.120077  0.769443  0.199073  0.849936   \n",
       "       group_2       0.831864  0.299137  0.678832  0.297463  0.758101   \n",
       "       group_3       0.942789  0.120787  0.767988  0.196788  0.849594   \n",
       "       group_4       0.935191  0.142607  0.765839  0.204035  0.846438   \n",
       "\n",
       "                                time              \n",
       "                     std        mean         std  \n",
       "type   group                                      \n",
       "manual group_1  0.121476  504.460182  232.729512  \n",
       "       group_2  0.104602  449.367549  291.407353  \n",
       "       group_3  0.089877  499.354180  177.042705  \n",
       "       group_4  0.107169  469.978607  255.482654  \n",
       "sam    group_1  0.179750  428.067545  205.231083  \n",
       "       group_2  0.289651  432.748965  243.642707  \n",
       "       group_3  0.176268  340.951425  166.866296  \n",
       "       group_4  0.185568  431.252484  286.737109  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"\\nGroup Statistics for Common Images:\")\n",
    "display(group_stats)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Statistical Tests (SAM vs Manual):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>t_statistic</th>\n",
       "      <th>p_value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>pixel_accuracy</th>\n",
       "      <td>-5.9215</td>\n",
       "      <td>3.6286e-09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>iou</th>\n",
       "      <td>-0.9743</td>\n",
       "      <td>3.3000e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dice</th>\n",
       "      <td>-3.5416</td>\n",
       "      <td>4.0503e-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>time</th>\n",
       "      <td>-7.9258</td>\n",
       "      <td>3.3855e-15</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               t_statistic     p_value\n",
       "pixel_accuracy     -5.9215  3.6286e-09\n",
       "iou                -0.9743  3.3000e-01\n",
       "dice               -3.5416  4.0503e-04\n",
       "time               -7.9258  3.3855e-15"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# Perform statistical tests\n",
    "from scipy import stats\n",
    "\n",
    "# Comparing SAM vs Manual for each metric\n",
    "statistical_tests = {}\n",
    "for metric in ['pixel_accuracy', 'iou', 'dice', 'time']:\n",
    "    sam_data = df[df['type'] == 'sam'][metric]\n",
    "    manual_data = df[df['type'] == 'manual'][metric]\n",
    "    t_stat, p_value = stats.ttest_ind(sam_data, manual_data)\n",
    "    statistical_tests[metric] = {\n",
    "        't_statistic': f\"{t_stat:.4f}\",\n",
    "        'p_value': f\"{p_value:.4e}\"\n",
    "    }\n",
    "\n",
    "print(\"\\nStatistical Tests (SAM vs Manual):\")\n",
    "display(pd.DataFrame(statistical_tests).T)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "analise de metricas "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Semantic segmentation (SemS).\n",
    "\n",
    "In semantic segmentation, classification occurs at pixel level.\n",
    "However, it is not advisable to simply apply the standard classification metrics to the entire collection\n",
    "of pixels in a data set for two reasons. Firstly, pixels of the same image are highly correlated. Hence,\n",
    "to respect the hierarchical data structure, metric values should first be computed per image and\n",
    "then be aggregated over the set of images. Note in this context that the commonly used DSC is\n",
    "mathematically identical to the popular F1 Score applied at pixel level. Secondly, in segmentation\n",
    "problems, the user typically has an inherent interest in structure boundaries, centers or volumes\n",
    "of structures (FP2.1, FP2.2, FP2.3). The family of boundary-based metrics (subset of distance-based\n",
    "metrics) therefore requires the extraction of structure boundaries from the binary segmentation\n",
    "masks as a foundation for segmentation assessment. Based on these considerations and given all the\n",
    "complementary strengths and weaknesses of common segmentation metrics [ 127 ], we recommend\n",
    "the following process for segmentation problems (yellow path in Fig. 2; detailed description in\n",
    "Suppl. Note 2.3):\n",
    "\n",
    "### 1: Select overlap-based metric (if any):\n",
    "In segmentation problems, counting metrics such\n",
    "as the **DSC** or **IoU** measure the overlap between the reference annotation and the algorithm\n",
    "prediction. As they can be considered the de facto standard for assessing segmentation\n",
    "quality and are well-interpretable, we recommend using them by default unless the target\n",
    "structures are consistently small, relative to the grid size (FP3.1), and the reference may be\n",
    "noisy (FP4.3.1). Depending on the specific properties of the problems, we recommend the DSC\n",
    "or IoU (default recommendation), the F𝛽 Score (preferred when there is a preference for either\n",
    "False Positive ( FP ) or False Negative ( FN )) or the centerline Dice Similarity Coefficient ( clDice)\n",
    "(for tubular structures). Details can be found in Subprocess S6 for selecting overlap-based\n",
    "metrics (Extended Data Fig. 6).16 Maier-Hein/Reinke et al.\n",
    "\n",
    "\n",
    "### 2: Select boundary-based metric (if any):\n",
    "\n",
    "Key weaknesses of overlap-based metrics include\n",
    "shape unawareness and limitations when dealing with small structures or high size variability [ 127 ]. Our general recommendation is therefore to complement an overlap-based\n",
    "metric with a boundary-based metric. If annotation imprecisions should be compensated\n",
    "for (FP2.5.7), **our default recommendation is the Normalized Surface Distance ( NSD )**. Otherwise, the fundamental user preference guiding metric selection is whether errors should\n",
    "be penalized by existence or distance (FP2.5.6), as detailed in Subprocess S7 for selecting\n",
    "boundary-based metrics (Extended Data Fig. 7)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extended Data Fig. SN 2.13. The Panoptic Quality ( PQ) measures the segmentation and detection quality\n",
    "of a prediction in one score. The metric simply averages the IoU scores for all True Positive ( TP) instances\n",
    "and multiplies the result with the F1 score. For perfect segmentation results, i.e., an average IoU of 1, the PQ\n",
    "would equal the F1 Score.https://arxiv.org/pdf/2206.01653#page=204.00"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "i am joining object detection + semantic segmentation which gives me instance segmentaiton "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gradio",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
